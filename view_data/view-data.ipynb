{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"Baseline\"\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kaggle.competitions import twosigmanews\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "\n",
    "print('Training lightgbm')\n",
    "\n",
    "# money\n",
    "PARAMS = {\"objective\":        \"binary\",\n",
    "          \"metric\":           \"binary_logloss\",\n",
    "          \"num_leaves\":       60,\n",
    "          \"max_depth\":        -1,\n",
    "          \"learning_rate\":    0.01,\n",
    "          \"bagging_fraction\": 0.9,  # subsample\n",
    "          \"feature_fraction\": 0.9,  # colsample_bytree\n",
    "          \"bagging_freq\":     5,  # subsample_freq\n",
    "          \"bagging_seed\":     2018,\n",
    "          \"verbosity\":        -1\n",
    "          }\n",
    "\n",
    "PARAMS_SVC = {\n",
    "    'C':                       1,\n",
    "    'decision_function_shape': 'ovr',\n",
    "    'kernel':                  'linear'\n",
    "    }\n",
    "\n",
    "RETURN_10_NEXT = 'returnsOpenNextMktres10'\n",
    "TIME = 'time'\n",
    "ASSET = 'assetCode'\n",
    "\n",
    "\n",
    "def prepare_data(market_df: pd.DataFrame, news_df: pd.DataFrame):\n",
    "    print('preparing data...')\n",
    "    # a bit of feature engineering\n",
    "    # TODO NEED ADD MA , SHIFTS\n",
    "    market_df[TIME] = market_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    market_df['bartrend'] = market_df['close'] / market_df['open']\n",
    "    market_df['average'] = (market_df['close'] + market_df['open']) / 2\n",
    "    market_df['pricevolume'] = market_df['volume'] * market_df['close']\n",
    "\n",
    "    news_df[TIME] = news_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n",
    "    news_df[ASSET] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n",
    "    news_df['position'] = news_df['firstMentionSentence'] / news_df[\n",
    "        'sentenceCount']\n",
    "    news_df['coverage'] = news_df['sentimentWordCount'] / news_df['wordCount']\n",
    "\n",
    "\n",
    "    # get rid of extra junk from news data\n",
    "    droplist = ['sourceTimestamp', 'firstCreated', 'sourceId', 'headline',\n",
    "                'takeSequence', 'provider', 'firstMentionSentence',\n",
    "                'sentenceCount', 'bodySize', 'headlineTag', 'marketCommentary',\n",
    "                'subjects', 'audiences', 'sentimentClass',\n",
    "                'assetName', 'assetCodes', 'urgency', 'wordCount',\n",
    "                'sentimentWordCount']\n",
    "    news_df.drop(droplist, axis=1, inplace=True)\n",
    "    market_df.drop(['assetName', 'volume'], axis=1, inplace=True)\n",
    "\n",
    "    # combine multiple news reports for same assets on same day\n",
    "    news_gp = news_df.groupby([TIME, ASSET], sort=False).aggregate(\n",
    "            np.mean).reset_index()\n",
    "\n",
    "    # join news reports to market data, note many assets will have many\n",
    "    # days without news data\n",
    "    return pd.merge(market_df, news_gp, how='left', on=[TIME, ASSET],\n",
    "                    copy=False).fillna(0)  # , right_on=['time', 'assetCodes'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ENV = twosigmanews.make_env()\n",
    "(market_df, news_df) = ENV.get_training_data()\n",
    "data_df = prepare_data(market_df, news_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('building training set...')\n",
    "\n",
    "def get_ans(all_data  ) -> pd.Series:\n",
    "    result = sum(_answer_sma(all_data , window)\n",
    "                     for window in range(10, 30 + 1, 1))\n",
    "\n",
    "    result = result.apply(np.sign)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def _answer_sma(all_data: pd.DataFrame , window ):\n",
    "    close = all_data['close']\n",
    "    f_sma = close.rolling(window=window, min_periods=window).mean()\n",
    "    b_sma = f_sma.shift(-window)\n",
    "    b_sma.fillna(close)\n",
    "\n",
    "    indicator = f_sma - b_sma\n",
    "    signal = _get_intersections(indicator, close)\n",
    "    return signal\n",
    "\n",
    "def _get_intersections(indicator, time_series):\n",
    "    indicator_shift = indicator.shift(1)\n",
    "\n",
    "    rolling = time_series.rolling(5, center=True)\n",
    "    range_ = np.arange(len(time_series) - 4)\n",
    "    # find the index number of rolling argmax and argmin\n",
    "\n",
    "    roll_argmax = rolling.apply(np.argmax)[2:-2].T.astype(int) + range_\n",
    "    roll_argmin = rolling.apply(np.argmin)[2:-2].T.astype(int) + range_\n",
    "\n",
    "    # find the index of buy and sell points (where two sma intersect)\n",
    "    # sell_index = result[(indicator >= 0) & (indicator_shift < 0)].index\n",
    "    # buy_index = result[(indicator < 0) & (indicator_shift >= 0)].index\n",
    "\n",
    "    # find local argmax and argmin in the buy and sell points\n",
    "    sell_index = roll_argmax[(indicator >= 0) & (indicator_shift < 0)]\n",
    "    buy_index = roll_argmin[(indicator < 0) & (indicator_shift >= 0)]\n",
    "\n",
    "    result = pd.Series(len(time_series) * [0], index=time_series.index)\n",
    "    result[result.index[sell_index]] = -1\n",
    "    result[result.index[buy_index]] = 1\n",
    "\n",
    "    return result.astype(int)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GROUP_COLS = [TIME,  RETURN_10_NEXT , ASSET,  'universe']\n",
    "\n",
    "TRAIN_COLS = [col for col in data_df.columns\n",
    "              if col not in GROUP_COLS]\n",
    "\n",
    "# dates = data_df[TIME].unique()\n",
    "\n",
    "# train = range(len(dates))[:int(0.95 * len(dates))]\n",
    "# train = data_df[TIME].isin(dates[train])\n",
    "\n",
    "# val = range(len(dates))[int(0.95 * len(dates)):]\n",
    "# val = data_df[TIME].isin(dates[val])\n",
    "\n",
    "d = data_df[data_df['universe'] == 1]\n",
    "tickers_df = d.groupby(ASSET)\n",
    "svms = {}\n",
    "scalers = {}\n",
    "for tiker, data_t in list(tickers_df)[:5]:\n",
    "        print(\"-----------\",tiker,\"------------------------------\")\n",
    "        s = SVC(**PARAMS_SVC)\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "        x = data_t[TRAIN_COLS]\n",
    "        y = get_ans(data_t)\n",
    "        x = scaler.fit_transform(x)\n",
    "        try:\n",
    "            s.fit(x,y)\n",
    "            svms[tiker] = s \n",
    "            scalers[tiker] = scaler\n",
    "        except  ValueError as error:\n",
    "            print(error)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "07b1a9d8eb673c2bd28327ea47af7e18b60f85cb"
   },
   "outputs": [],
   "source": [
    "pred_days = ENV.get_prediction_days()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "653c87d4e52aa19b2b7d224e7cbe3fcec86cb987"
   },
   "outputs": [],
   "source": [
    "for market_df, news_df, pred_template_df in pred_days:\n",
    "    data_df_predict = prepare_data(market_df, news_df)\n",
    "    result = []\n",
    "    for asset in pred_template_df[ASSET]:\n",
    "        data_df_predict_a = data_df_predict[data_df_predict[ASSET] ==  asset]\n",
    "        data_df_predict_a = data_df_predict_a[TRAIN_COLS]\n",
    "        try:\n",
    "            x = scalers[asset].transform(data_df_predict_a)\n",
    "            result.append(svms[asset].predict(x))\n",
    "        except KeyError as error:\n",
    "            print(error) \n",
    "            result.append(0)\n",
    "    pred_template_df['confidenceValue'] = result\n",
    "    ENV.predict(pred_template_df)\n",
    "    print(pred_template_df)\n",
    "        \n",
    "\n",
    "ENV.write_submission_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "7f8010a300173d1411aff54886128fa696a112c7"
   },
   "outputs": [],
   "source": [
    "ENV.predict(pred_template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "13c22f493c7fa38c30a9d919d0e2b6c784a947a0"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
